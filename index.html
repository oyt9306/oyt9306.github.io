<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Yeongtak Oh</title>
  
  <meta name="author" content="Yeongtak Oh">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <div class="container">
    
    <header class="profile-section">
      <div class="profile-text">
        <h1 class="name">Yeongtak Oh <span class="korean-name">(Ïò§ÏòÅÌÉÅ)</span></h1>
        <p class="bio">
          Hi, I'm a fourth-year Ph.D. candidate in ECE at Seoul National University, working in the <a href="https://dsail.snu.ac.kr">DSAIL Lab</a>. I research computer vision and multi-modal reasoning. My work primarily explores post-training of generative models. I'm deeply interested in advancing multi-modal AI systems in more expressive and personalized ways.
        </p>
        <p class="bio">
          I received my B.S. (2018) and M.S. (2020) degrees in Mechanical Engineering from Seoul National University. In 2021, I served as a Military Science and Technology Researcher at the AI R&D Center of the Korea Military Academy.
        </p>
        <div class="social-links">
          <a href="mailto:oyt9306@gmail.com">Email</a>
          <a href="data/CV_YeongtakOh.pdf">CV</a>
          <a href="https://scholar.google.co.kr/citations?user=1251qTIAAAAJ&hl=ko">Google Scholar</a>
          <a href="https://github.com/oyt9306">Github</a>
        </div>
      </div>
      <div class="profile-image-wrapper">
        <img src="images/profile_2.jpg" alt="Yeongtak Oh" class="profile-img">
      </div>
    </header>

    <hr class="divider">

    <section class="section news">
      <h2 class="section-title">News</h2>
      <ul class="news-list">
        <li><span class="date">[2025/11]</span> Two papers got accepted to <b>WACV 2026</b> Conference!</li>
        <li><span class="date">[2025/09]</span> One paper got accepted to <b>NeurIPS 2025</b> Conference!</li>
        <li><span class="date">[2024/11]</span> One paper got accepted to <b>IJCV 2024</b> Journal!</li>
        <li><span class="date">[2024/07]</span> One paper got accepted to <b>BMVC 2024</b> Conference!</li>
        <li><span class="date">[2024/07]</span> One paper got accepted to <b>ECCV 2024</b> Conference</li>
      </ul>
    </section>

    <section class="section publications">
      <h2 class="section-title">Conferences</h2>

      <h3 class="year-heading">2026</h3>
      
      <article class="paper-entry">
        <div class="paper-img-container">
          <img src="images/NPC.png" alt="NPC" class="paper-img">
        </div>
        <div class="paper-content">
          <a href="https://openreview.net/forum?id=tlYNuAGmOY&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dthecvf.com%2FWACV%2F2026%2FConference%2FAuthors%23your-submissions)" class="paper-title">
            Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment
          </a>
          <p class="authors">
            Sangha Park, Eunji Kim, <span class="me">Yeongtak Oh</span>, Jooyoung Choi, and Sungroh Yoon
          </p>
          <p class="venue">The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2026</p>
          <div class="links">
            <a href="https://openreview.net/forum?id=tlYNuAGmOY&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dthecvf.com%2FWACV%2F2026%2FConference%2FAuthors%23your-submissions)">OpenReview</a>
          </div>
          <p class="abstract">We introduces Negative Prompting for Image Correction (NPC), an automated pipeline for using negative prompts to enhance image-text alignment.</p>
        </div>
      </article>

      <article class="paper-entry">
        <div class="paper-img-container">
          <img src="images/style_friendly.jpg" alt="Style Friendly" class="paper-img">
        </div>
        <div class="paper-content">
          <a href="https://arxiv.org/abs/2411.14793" class="paper-title">
            Style-Friendly SNR Sampler for Style-Driven Generation
          </a>
          <p class="authors">
            Jooyoung Choi*, Chaehun Shin*, <span class="me">Yeongtak Oh</span>, Heeseung Kim, and Sungroh Yoon
          </p>
          <p class="venue">The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2026</p>
          <div class="links">
            <a href="https://stylefriendly.github.io/">project page</a> / 
            <a href="https://arxiv.org/abs/2411.14793">arXiv</a>
          </div>
          <p class="abstract">We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge.</p>
        </div>
      </article>

      <h3 class="year-heading">2025</h3>

      <article class="paper-entry">
        <div class="paper-img-container">
          <img src="images/overall_RePIC.png" alt="RePIC" class="paper-img">
        </div>
        <div class="paper-content">
          <a href="https://arxiv.org/abs/2506.18369" class="paper-title">
            RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models
          </a>
          <p class="authors">
            <span class="me">Yeongtak Oh</span>, Dohyun Chung, Juhyeon Shin, Sangha Park, Johan Barthelemy, Jisoo Mok*, and Sungroh Yoon*
          </p>
          <p class="venue">Neural Information Processing Systems (NeurIPS)</p>
          <p class="highlight-text">
            Selected as a Finalist in Qualcomm Innovation Fellowship Korea (QIFK) 2025 <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/finalists"><b>(Link)</b></a>
          </p>
          <div class="links">
            <a href="https://github.com/oyt9306/RePIC">project page</a> / 
            <a href="https://arxiv.org/abs/2506.18369">arXiv</a>
          </div>
          <p class="abstract">We propose RePIC, a reinforced post-training framework that outperforms SFT-based methods in multi-concept personalized image captioning by enhancing visual recognition and generalization through reward templates and curated instructions.</p>
        </div>
      </article>

      <h3 class="year-heading">2024</h3>

      <article class="paper-entry">
        <div class="paper-img-container">
          <img src="images/controldreamer.jpg" alt="ControlDreamer" class="paper-img">
        </div>
        <div class="paper-content">
          <a href="https://controldreamer.github.io/" class="paper-title">
            ControlDreamer: Stylized 3D Generation with Multi-View ControlNet
          </a>
          <p class="authors">
            <span class="me">Yeongtak Oh*</span>, Jooyoung Choi*, Yongsung Kim, Minjun Park, Chaehun Shin, and Sungroh Yoon
          </p>
          <p class="venue">British Machine Vision Conference (BMVC)</p>
          <div class="links">
            <a href="https://controldreamer.github.io/">project page</a> / 
            <a href="https://arxiv.org/abs/2312.01129">arXiv</a>
          </div>
          <p class="abstract">ControlDreamer enables high-quality 3D generation with creative geometry and styles via multi-view ControlNet.</p>
        </div>
      </article>

      <article class="paper-entry">
        <div class="paper-img-container">
          <img src="images/decorruptor.jpg" alt="Decorruptor" class="paper-img">
        </div>
        <div class="paper-content">
          <a href="https://arxiv.org/pdf/2403.10911.pdf" class="paper-title">
            Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation
          </a>
          <p class="authors">
            <span class="me">Yeongtak Oh*</span>, Jonghyun Lee*, Jooyoung Choi, Uiwon Hwang, Dahuin Jung, and Sungroh Yoon
          </p>
          <p class="venue">European Conference on Computer Vision (ECCV)</p>
          <div class="links">
            <a href="https://github.com/oyt9306/Decorruptor">project page</a> / 
            <a href="https://arxiv.org/abs/2403.10911">arXiv</a>
          </div>
          <p class="abstract">We propose Decorruptor to enhance the robustness of the diffusion model and accelerate the diffusion-based image-level updates.</p>
        </div>
      </article>

      <h2 class="section-title">Journals</h2>
      
      <h3 class="year-heading">2024</h3>
       <article class="paper-entry">
        <div class="paper-img-container">
          <img src="images/main.jpeg" alt="Geodesic Distillation" class="paper-img">
        </div>
        <div class="paper-content">
          <a href="https://arxiv.org/pdf/2401.10526.pdf" class="paper-title">
            On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss
          </a>
          <p class="authors">
            <span class="me">Yeongtak Oh</span>, Saehyung Lee, Uiwon Hwang*, and Sungroh Yoon*
          </p>
          <p class="venue">International Journal of Computer Vision (IJCV), <u>IF: 11.6</u></p>
          <div class="links">
            <a href="https://github.com/oyt9306/geodesic-CLIP">project page</a> / 
            <a href="https://arxiv.org/pdf/2401.10526.pdf">arXiv</a>
          </div>
          <p class="abstract">We have enhanced a range of CLIP-guided image morphing baselines through the implementation of our proposed inter- and intra-modality regularization losses.</p>
        </div>
      </article>

      <h3 class="year-heading">2022</h3>
      <article class="paper-entry">
       <div class="paper-img-container">
         <img src="images/dtma_overview.jpg" alt="Fault Detection" class="paper-img">
       </div>
       <div class="paper-content">
         <a href="https://www.sciencedirect.com/science/article/pii/S0019057821005899" class="paper-title">
           A deep transferable motion-adaptive fault detection method for industrial robots using a residual‚Äìconvolutional neural network
         </a>
         <p class="authors">
           <span class="me">Yeongtak Oh</span>, Yunhan Kim, Kyumin Na, and Byeng D. Youn
         </p>
         <p class="venue">ISA Transactions, <u>IF: 5.9</u></p>
         <p class="abstract">We present a deep learning-based motion-adaptive fault detection method for industrial robots using torque ripples.</p>
       </div>
     </article>
    </section>

    <section class="section talks">
      <h2 class="section-title">Talks</h2>
      <ul class="talk-list">
        <li>
          <span class="date">[2023.08.25]</span>
          <a href="https://drive.google.com/file/d/1_dbjySNl0oDJxaionc6TtLJNqGOM6M_d/view?usp=sharing">Recent Trends of Generative models in 3D vision</a>
        </li>
        <li>
          <span class="date">[2024.11.27]</span>
          <a href="https://drive.google.com/file/d/1eijsEbBvf3M30DfaHcYXIr4s1s0dc0fM/view?usp=sharing">Image-Inversion of Diffusion Models</a>
        </li>
      </ul>
    </section>

    <footer class="footer">
      <p>
        Design inspired by <a href="https://jonbarron.info/">Jon Barron</a>'s website.
      </p>
    </footer>

  </div>
</body>
</html>
