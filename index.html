<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yeongtak Oh</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yeongtak Oh</name>
              </p>
              <p>
              Hi, I'm a fourth-year Ph.D. candidate in ECE at Seoul National University, working in the <a href="https://dsail.snu.ac.kr">DSAIL Lab</a>. I research computer vision and multi-modal reasoning. My work primarily explores post-training of generative models. I'm deeply interested in advancing multi-modal AI systems in more expressive and personalized ways. 
              <br>
              <br>
              I received my B.S. (2018) and M.S. (2020) degrees in Mechanical Engineering from Seoul National University. In 2021, I served as a Military Science and Technology Researcher at the AI R&D Center of the Korea Military Academy.
              </p>
              <p style="text-align:center">
                <a href="mailto:oyt9306@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_YeongtakOh.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.kr/citations?user=1251qTIAAAAJ&hl=ko">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/oyt9306">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="images/profile_mod.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_mod.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

      <h2>News</h2>
        [2025/09] One paper got accepted to <b>NeurIPS 2025</b> Conference!
        <br>
        [2024/11] One paper got accepted to <b>IJCV 2024</b> Journal!
        <br>
        [2024/07] One paper got accepted to <b>BMVC 2024</b> Conference!
        <br>
        [2024/07] One paper got accepted to <b>ECCV 2024</b> Conference
      <h2>Conferences</h2>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
                         <div class="one">
                          <img src="images/overall_RePIC.png" alt="b3do" width="180" style="border-style: none">
                        </div>
                        <script type="text/javascript">
                          function db3d_start() {
                            document.getElementById('db3d_image').style.opacity = "1";
                          }

                          function db3d_stop() {
                            document.getElementById('db3d_image').style.opacity = "0";
                          }
                          db3d_stop()
                        </script>
            </td> 
        <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2506.18369">
                <span class="papertitle">RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models</span>
              </a>
              <br>
              <b>Yeongtak Oh</b>, Jisoo Mok, Dohyun Chung, Juhyeon Shin, Sangha Park, Johan Barthelemy, and Sungroh Yoon
              <br>
              <em style="color:Salmon;">Neural Information Processing Systems (NeurIPS)</em>, 2025
              <br>
              <a href="https://github.com/oyt9306/RePIC">project page</a> / 
              <a href="https://arxiv.org/abs/2506.18369">arXiv</a>
              <p></p>
          <p>We propose RePIC, a reinforced post-training framework that outperforms SFT-based methods in multi-concept personalized image captioning by enhancing visual recognition and generalization through reward templates and curated instructions.

          </p>
        </td>
      </tr>
    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
   <td style="padding:20px;width:25%;vertical-align:middle">
               <div class="one">
                <img src="images/controldreamer.jpg" alt="b3do" width="180" style="border-style: none">
              </div>
              <script type="text/javascript">
                function db3d_start() {
                  document.getElementById('db3d_image').style.opacity = "1";
                }

                function db3d_stop() {
                  document.getElementById('db3d_image').style.opacity = "0";
                }
                db3d_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://controldreamer.github.io/">
                          <span class="papertitle">ControlDreamer: Stylized 3D Generation
          with Multi-View ControlNet</span>
                        </a>
                        <br>
                        <b>Yeongtak Oh*</b>, Jooyoung Choi*, Yongsung Kim, Minjun Park, Chaehun Shin, and Sungroh Yoon
                        <br>
                        * Equal Contribution
                        <br>
                        <em style="color:Salmon;">British Machine Vision Conference (BMVC)</em>, 2024
                        <br>
                        <a href="https://controldreamer.github.io/">project page</a> / 
                        <a href="https://arxiv.org/abs/2312.01129">arXiv</a>
                        <p></p>
                        <p>ControlDreamer enables high-quality 3D generation with creative geometry and styles via multi-view ControlNet.</p>
            </td>           
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
                         <div class="one">
                          <img src="images/decorruptor.jpg" alt="b3do" width="180" style="border-style: none">
                        </div>
                        <script type="text/javascript">
                          function db3d_start() {
                            document.getElementById('db3d_image').style.opacity = "1";
                          }

                          function db3d_stop() {
                            document.getElementById('db3d_image').style.opacity = "0";
                          }
                          db3d_stop()
                        </script>
            </td> 
        <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2403.10911.pdf">
                <span class="papertitle">Efficient Diffusion-Driven Corruption Editor for Test-Time Adaptation</span>
              </a>
              <br>
              <b>Yeongtak Oh*</b>, Jonghyun Lee*, Jooyoung Choi, Uiwon Hwang, Dahuin Jung, and Sungroh Yoon
        <br>
              * Equal Contribution
              <br>
              <em style="color:Salmon;">European Conference on Computer Vision (ECCV)</em>, 2024
              <br>
              <a href="https://github.com/oyt9306/Decorruptor">project page</a> / 
              <a href="https://arxiv.org/abs/2403.10911">arXiv</a>
              <p></p>
          <p>We propose Decorruptor to enhance the robustness of the diffusion model and accelerate the diffusion-based image-level updates.</p>

  </td>
    </tr>
      </table>


      <h2>Journals</h2>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
   <td style="padding:20px;width:25%;vertical-align:middle">
               <div class="one">
                <img src="images/main.jpeg" alt="b3do" width="180" style="border-style: none">
              </div>
              <script type="text/javascript">
                function db3d_start() {
                  document.getElementById('db3d_image').style.opacity = "1";
                }

                function db3d_stop() {
                  document.getElementById('db3d_image').style.opacity = "0";
                }
                db3d_stop()
              </script>
            </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2401.10526.pdf">
                <span class="papertitle">On mitigating stability-plasticity dilemma in CLIP-guided image morphing
via geodesic distillation loss</span>
              </a>
              <br>
              <b>Yeongtak Oh</b>, Saehyung Lee, Uiwon Hwang*, and Sungroh Yoon*
              <br>
              * Equal Corresponding
              <br>
              <em style="color:Salmon;">International Journal of Computer Vision (IJCV)</em>, <u>IF: 11.6</u>, 2024
              <br>
              <a href="https://github.com/oyt9306/geodesic-CLIP">project page</a> / 
              <a href="https://arxiv.org/pdf/2401.10526.pdf">arXiv</a>
              <p></p>
              <p>We have enhanced a range of CLIP-guided image morphing baselines through the implementation of our proposed inter- and intra-modality regularization losses, effectively addressing the SP dilemma.</p>           
    </tr>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src="images/dtma_overview.jpg" alt="b3do" width="160" style="border-style: none">
            </div>
            <script type="text/javascript">
              function db3d_start() {
                document.getElementById('db3d_image').style.opacity = "1";
              }

              function db3d_stop() {
                document.getElementById('db3d_image').style.opacity = "0";
              }
              db3d_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.sciencedirect.com/science/article/pii/S0019057821005899">
              <span class="papertitle">A deep transferable motion-adaptive fault detection method for industrial robots using a residual‚Äìconvolutional neural network</span>
            </a>
            <br>
            <b>Yeongtak Oh</b>, Yunhan Kim, Kyumin Na, and Byeng D. Youn
            <br>
            <em style="color:Salmon;">ISA Transactions</em>, <u>IF: 5.9</u>, 2022
            <br>
            <p></p>
            <p>We present a deep learning-based motion-adaptive fault detection method for industrial robots using torque ripples.</p>
          </td>
        </tr>
      </table>
      <h2>Preprints</h2>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
                         <div class="one">
                          <img src="images/style_friendly.jpg" alt="b3do" width="180" style="border-style: none">
                        </div>
                        <script type="text/javascript">
                          function db3d_start() {
                            document.getElementById('db3d_image').style.opacity = "1";
                          }

                          function db3d_stop() {
                            document.getElementById('db3d_image').style.opacity = "0";
                          }
                          db3d_stop()
                        </script>
            </td> 
        <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2411.14793">
                <span class="papertitle">Style-Friendly SNR Sampler for Style-Driven Generation</span>
              </a>
              <br>
              Jooyoung Choi*, Chaehun Shin*, <b>Yeongtak Oh</b>, Heeseung Kim, and Sungroh Yoon
        <br>
              * Equal Contribution
              <br>
              arxiv, 2024
              <br>
              <a href="">project page</a> / 
              <a href="https://arxiv.org/abs/2411.14793">arXiv</a>
              <p></p>
          <p>We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <h2>Talks</h2>
  [2023.08.25] <a href="https://drive.google.com/file/d/1_dbjySNl0oDJxaionc6TtLJNqGOM6M_d/view?usp=sharing">Recent Trends of Generative models in 3D vision</a>
  <br>
  [2024.11.27] <a href="https://drive.google.com/file/d/1eijsEbBvf3M30DfaHcYXIr4s1s0dc0fM/view?usp=sharing">Image-Inversion of Diffusion Models</a>
  <br>
    <!-- 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/safs.jpg" alt="safs_small" width="160" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1EZTOO5xezLYcyIFgAzs4KuZFLbTcwTDH/view?usp=sharing">
                <papertitle>High-Frequency Shape and Albedo from Shading using Natural Image Statistics</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>CVPR</em>, 2011
              <br>
              <a href="data/BarronMalikCVPR2011.bib">bibtex</a>
              <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
            </td>
          </tr> -->


<!--         </tbody></table>

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Bibliography</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
            </td>
          </tr>
          
          
        </tbody></table> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This page is borrowed from <a href="https://jonbarron.info/">Jon Barron's webpage</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
